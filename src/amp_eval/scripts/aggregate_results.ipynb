{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amp Model Evaluation Results Aggregation\n",
    "\n",
    "This notebook aggregates and analyzes results from the model evaluation suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation results\n",
    "def load_eval_results(results_dir=\"../results\"):\n",
    "    \"\"\"Load all evaluation result files.\"\"\"\n",
    "    results_path = Path(results_dir)\n",
    "    all_results = []\n",
    "    \n",
    "    for result_file in results_path.glob(\"*.json\"):\n",
    "        with open(result_file) as f:\n",
    "            data = json.load(f)\n",
    "            data['eval_name'] = result_file.stem\n",
    "            all_results.append(data)\n",
    "    \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate metrics by model\n",
    "def aggregate_by_model(results):\n",
    "    \"\"\"Group results by model and calculate aggregate metrics.\"\"\"\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Group by model and calculate metrics\n",
    "    model_stats = df.groupby('model').agg({\n",
    "        'success_rate': 'mean',\n",
    "        'latency_s': ['mean', 'std'],\n",
    "        'tokens': ['mean', 'std'],\n",
    "        'eval_name': 'count'\n",
    "    }).round(3)\n",
    "    \n",
    "    model_stats.columns = ['success_rate', 'avg_latency', 'latency_std', \n",
    "                          'avg_tokens', 'tokens_std', 'total_runs']\n",
    "    \n",
    "    return model_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display results\n",
    "try:\n",
    "    results = load_eval_results()\n",
    "    if results:\n",
    "        model_comparison = aggregate_by_model(results)\n",
    "        print(\"Model Performance Summary:\")\n",
    "        print(model_comparison)\n",
    "    else:\n",
    "        print(\"No evaluation results found. Run evaluations first.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading results: {e}\")\n",
    "    print(\"Make sure to run evaluations first to generate results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization functions\n",
    "def plot_model_comparison(model_stats):\n",
    "    \"\"\"Create comparison plots for model performance.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # Success rate comparison\n",
    "    axes[0,0].bar(model_stats.index, model_stats['success_rate'])\n",
    "    axes[0,0].set_title('Success Rate by Model')\n",
    "    axes[0,0].set_ylabel('Success Rate')\n",
    "    \n",
    "    # Latency comparison  \n",
    "    axes[0,1].bar(model_stats.index, model_stats['avg_latency'])\n",
    "    axes[0,1].set_title('Average Latency by Model')\n",
    "    axes[0,1].set_ylabel('Latency (seconds)')\n",
    "    \n",
    "    # Token usage comparison\n",
    "    axes[1,0].bar(model_stats.index, model_stats['avg_tokens'])\n",
    "    axes[1,0].set_title('Average Tokens by Model')\n",
    "    axes[1,0].set_ylabel('Tokens')\n",
    "    \n",
    "    # Total runs\n",
    "    axes[1,1].bar(model_stats.index, model_stats['total_runs'])\n",
    "    axes[1,1].set_title('Total Evaluation Runs')\n",
    "    axes[1,1].set_ylabel('Number of Runs')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate plots if we have data\n",
    "if 'model_comparison' in locals() and not model_comparison.empty:\n",
    "    fig = plot_model_comparison(model_comparison)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data available for plotting. Run evaluations first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. Run the evaluation suite: `openai tools evaluate amp-eval/evals/tool_calling_micro.yaml --registry amp-eval/adapters`\n",
    "2. Rerun this notebook to see the results\n",
    "3. Add more sophisticated analysis as needed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
